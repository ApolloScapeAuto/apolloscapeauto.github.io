# Challenge

Our the open-access data set, [ApolloScape](http://apolloscape.auto/), is a part of the Apollo project, the only open source autonomous driving platform which was initiated by Baidu Inc in 2017. In order to capture the static 3D world with high granularity, we used a mobile LIDAR scanner from Riegl to collect point clouds, which yields point cloud densities much higher than those from Velodyne (which was used by KITTI). In addition, two high-resolution cameras at the head of the collection car were synchronized and calibrated, recording at a frame rate of 30 fps. Each camera has high precision GPS and IMU, so that the accurate camera pose is recorded on-the-fly. All our videos were recorded from cities, e.g. Beijing, Shanghai and Shenzhen, in China.

![Challenge%2097323c84d46042a6ae35d43a66909471/scape_b188adb.png](Challenge%2097323c84d46042a6ae35d43a66909471/scape_b188adb.png)

In Tab. 1, we show the properties of our dataset compared to existing ones. In particular, we have high-quality 3D labels from realistic scenes for both the static background and moving objects. Currently, we already have 50K images labeled covering around 10 km from three sites in three cities. Moreover, each area was scanned repeatedly under various weather and lighting conditions. Finally, ApolloScape will be an evolving dataset and labeled data from new cities will be added monthly. We plan to have at least 200K images, consisting of 20 km road covering 5 sites from three cities, for holding the challenge. In the following, we will introduce the details of each challenge specifically targeting autonomous driving.

For all the challenges, in addition to testing the accuracy (which we will use to rank the algorithms), we also require the participants to specify the speed of the algorithm, the platform they use, and implementation details. We will encourage algorithms which run in real-time, i.e. 30 fps, and will highlight them in the leader-board, since speed is a crucial property for practical applications.

![Challenge%2097323c84d46042a6ae35d43a66909471/task1_8034025.jpg](Challenge%2097323c84d46042a6ae35d43a66909471/task1_8034025.jpg)

![Challenge%2097323c84d46042a6ae35d43a66909471/task2_0_c61ade6.png](Challenge%2097323c84d46042a6ae35d43a66909471/task2_0_c61ade6.png)

![Challenge%2097323c84d46042a6ae35d43a66909471/task3_8ac8a14.jpg](Challenge%2097323c84d46042a6ae35d43a66909471/task3_8ac8a14.jpg)

[1] Kendall Alex, Grimes Matthew, Cipolla Roberto, PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization 2015, ICCV

[2] Brostow Gabriel J, Fauqueur Julien, Cipolla Roberto, Semantic object classes in video: A high-definition ground truth database 2009, PR

[3] Geiger Andreas, Lenz Philip, Urtasun Raquel, Are we ready for autonomous driving? the kitti vision benchmark suite 2012, CVPR

[4] Cordts Marius, Omran Mohamed, Ramos Sebastian, Rehfeld Timo, Enzweiler Markus, Benenson Rodrigo, Franke Uwe, Roth Stefan, Schiele Bernt, The Cityscapes Dataset for Semantic Urban Scene Understanding 2016, CVPR

[5] CWang Shenlong, Bai Min, Mattyus Gellert, Chu Hang, Luo Wenjie, Yang Bin, Liang Justin, Cheverie Joel, Fidler Sanja, Urtasun Raquel, TorontoCity: Seeing the World with a Million Eyes 2017, ICCV

[6] German Ros, Laura Sellart, Joanna Materzynska,David Vazquez, Antonio Lopezt, The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes 2016, CVPR

[7] Richter Stephan R, Hayder Zeeshan, Koltun Vladlen, Playing for benchmarks 2017, ICCV

[8] Peng Wang, Ruigang Yang, Binbin Cao, Wei Xu, Yuanqing Lin, DeLS-3D: Deep Localization and Segmentation with a 3D Semantic Map 2018, CVPR